{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Boston Housing Prices\n",
    "\n",
    "## Using XGBoost in SageMaker (Batch Transform)\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "As an introduction to using SageMaker's Low Level Python API we will look at a relatively simple problem. Namely, we will use the [Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) to predict the median value of a home in the area of Boston Mass.\n",
    "\n",
    "The documentation reference for the API used in this notebook is the [SageMaker Developer's Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/)\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Typically, when using a notebook instance with SageMaker, you will proceed through the following steps. Of course, not every step will need to be done with each project. Also, there is quite a lot of room for variation in many of the steps, as you will see throughout these lessons.\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n",
    "\n",
    "In this notebook we will only be covering steps 1 through 5 as we just want to get a feel for using SageMaker. In later notebooks we will talk about deploying a trained model in much more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.39)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.39 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.39)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.39->boto3>=1.14.12->sagemaker==1.72.0) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.39->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setting up the notebook\n",
    "\n",
    "We begin by setting up all of the necessary bits required to run our notebook. To start that means loading all of the Python modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the modules above, we need to import the various bits of SageMaker that we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# This is an object that represents the SageMaker session that we are currently operating in. This\n",
    "# object contains some useful information that we will need to access later such as our region.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# This is an object that represents the IAM role that we are currently assigned. When we construct\n",
    "# and launch the training job later we will need to tell it what IAM role it should have. Since our\n",
    "# use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data\n",
    "\n",
    "Fortunately, this dataset can be retrieved using sklearn and so this step is relatively straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing and splitting the data\n",
    "\n",
    "Given that this is clean tabular data, we don't need to do any processing. However, we do need to split the rows in the dataset up into train, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we package up the input data and the target variable (the median value) as pandas dataframes. This\n",
    "# will make saving the data to a file a little easier later on.\n",
    "\n",
    "X_bos_pd = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "Y_bos_pd = pd.DataFrame(boston.target)\n",
    "\n",
    "# We split the dataset into 2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size=0.33)\n",
    "\n",
    "# Then we split the training set further into 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Uploading the data files to S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "### Save the data locally\n",
    "\n",
    "First we need to create the test, train and validation csv files which we will then upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our local data directory. We need to make sure that it exists.\n",
    "## we're saving the Data to the Boston subdirectory of the Data direction\n",
    "data_dir = '../data/boston'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to save our test, train and validation data to csv files. Note that we make sure not to include header\n",
    "# information or an index as this is required by the built in algorithms provided by Amazon. Also, for the train and\n",
    "# validation data, it is assumed that the first entry in each row is the target variable.\n",
    "\n",
    "## Now the files have been saved on the notebook instance, and then we can upload them to S3\n",
    "\n",
    "X_test.to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "Since we are currently running inside of a SageMaker session, we can use the object which represents this session to upload our data to the 'default' S3 bucket. Note that it is good practice to provide a custom prefix (essentially an S3 folder) to make sure that you don't accidentally interfere with data uploaded from some other notebook or project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'boston-xgboost-LL'\n",
    "\n",
    "## S3 is Amazon's data storage facility, it provides a place for you to store data that you may want to read many times, \n",
    "## for example: training data or a saved model\n",
    "\n",
    "## we use the \"upload_data\" data method of the session object\n",
    "## now we have uploaded our trainning and validation data to S3!!!\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and construct the XGBoost model\n",
    "\n",
    "Now that we have the training and validation data uploaded to S3, we can construct a training job for our XGBoost model and build the model itself.\n",
    "\n",
    "### Set up the training job\n",
    "\n",
    "First, we will set up and execute a training job for our model. To do this we need to specify some information that SageMaker will use to set up and properly execute the computation. For additional documentation on constructing a training job, see the [CreateTrainingJob API](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html) reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "### 这里的model指的是可以被 SageMaker 理解的model!!! which is a collection of information including any model artifacts created\n",
    "### during trainning and additional information describing how the model artifacts should be used  \n",
    "\n",
    "### 1. which algorithm to use, \n",
    "### 2. the parameters specific to that algorithm\n",
    "### 3. the data that is sent to the algorithm\n",
    "\n",
    "# We will need to know the name of the container that we want to use for training. SageMaker provides\n",
    "# a nice utility method to construct this for us.\n",
    "\n",
    "### we need to use the URL to the docker container containing XGBoost algorithm\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "\n",
    "# We now specify the parameters we wish to use for our training job\n",
    "training_params = {}\n",
    "\n",
    "# We need to specify the permissions that this training job will have. For our purposes we can use\n",
    "# the same permissions that our current SageMaker session has.\n",
    "\n",
    "### we need to specify what role the vitual machine that we will eventually create should have\n",
    "training_params['RoleArn'] = role\n",
    "\n",
    "# Here we describe the algorithm we wish to use. The most important part is the container which\n",
    "# contains the training code.\n",
    "\n",
    "### we need to specify the algorithm we wish to run\n",
    "training_params['AlgorithmSpecification'] = {\n",
    "    \"TrainingImage\": container, ### we do that by specifying a link to the container\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# We also need to say where we would like the resulting model artifacts stored.\n",
    "\n",
    "### we need to specify where the resulting model should be saved\n",
    "training_params['OutputDataConfig'] = {  ### this output of the algorithm is what is called the model artifacts\n",
    "    \"S3OutputPath\": \"s3://\" + session.default_bucket() + \"/\" + prefix + \"/output\"\n",
    "}\n",
    "\n",
    "# We also need to set some parameters for the training job itself. Namely we need to describe what sort of\n",
    "# compute instance we wish to use along with a stopping condition to handle the case that there is\n",
    "# some sort of error and the training script doesn't terminate.\n",
    "\n",
    "### we need to describe what properties the virtual machine we are going to run our algorithm on should have\n",
    "training_params['ResourceConfig'] = {\n",
    "    \"InstanceCount\": 1,  ### we are going to use a single virtual machine \n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 5\n",
    "}\n",
    "    \n",
    "training_params['StoppingCondition'] = { ### this parameter sets a timeout condition \n",
    "    \"MaxRuntimeInSeconds\": 86400\n",
    "}\n",
    "\n",
    "# Next we set the algorithm specific hyperparameters. You may wish to change these to see what effect\n",
    "# there is on the resulting model.\n",
    "\n",
    "### we need to set the parameters that are specific to the algorithm we've chosen\n",
    "training_params['HyperParameters'] = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"early_stopping_rounds\": \"10\",\n",
    "    \"num_round\": \"200\"\n",
    "}\n",
    "\n",
    "# Now we need to tell SageMaker where the data should be retrieved from.\n",
    "\n",
    "### we need to tell the SageMaker where to pull the data from\n",
    "### where the trainning data, validation data is\n",
    "training_params['InputDataConfig'] = [\n",
    "    {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\", ### where is the trainning data (S3 in this case)\n",
    "                \"S3Uri\": train_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",  ### it's CSV data\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    {\n",
    "        \"ChannelName\": \"validation\", \n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\", ### where is the validation data (S3 in this case)\n",
    "                \"S3Uri\": val_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the training job\n",
    "\n",
    "Now that we've built the dictionary object containing the training job parameters, we can ask SageMaker to execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to choose a training job name. This is useful for if we want to recall information about our\n",
    "# training job at a later date. Note that SageMaker requires a training job name and that the name needs to\n",
    "# be unique, which we accomplish by appending the current timestamp.\n",
    "\n",
    "### all the left is to ask the SageMaker to to construct the trainning job with the parameters we have chosen\n",
    "### every trainning job must have a name and that name must be unique, so we including a timestamp \"%Y-%m-%d-%H-%M-%S\" 时间\n",
    "### at the end of the name\n",
    "training_job_name = \"boston-xgboost-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "training_params['TrainingJobName'] = training_job_name\n",
    "\n",
    "# And now we ask SageMaker to create (and execute) the training job\n",
    "training_job = session.sagemaker_client.create_training_job(**training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job has now been created by SageMaker and is currently running. Since we need the output of the training job, we may wish to wait until it has finished. We can do so by asking SageMaker to output the logs generated by the training job and continue doing so until the training job terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-11 07:18:51 Starting - Starting the training job...\n",
      "2021-04-11 07:18:52 Starting - Launching requested ML instances......\n",
      "2021-04-11 07:20:14 Starting - Preparing the instances for training.........\n",
      "2021-04-11 07:21:27 Downloading - Downloading input data...\n",
      "2021-04-11 07:22:21 Training - Training image download completed. Training in progress.\n",
      "2021-04-11 07:22:21 Uploading - Uploading generated training model.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-04-11:07:22:16:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-04-11:07:22:16:INFO] File size need to be processed in the node: 0.03mb. Available memory size in the node: 8412.71mb\u001b[0m\n",
      "\u001b[34m[2021-04-11:07:22:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[07:22:16] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[07:22:16] 227x13 matrix with 2951 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-04-11:07:22:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[07:22:16] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[07:22:16] 112x13 matrix with 1456 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:19.2926#011validation-rmse:18.1841\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-rmse hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:15.7867#011validation-rmse:14.7191\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:12.9535#011validation-rmse:11.9501\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:10.7396#011validation-rmse:9.8479\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:8.9077#011validation-rmse:8.17991\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:7.4531#011validation-rmse:6.90205\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:6.36704#011validation-rmse:5.88586\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:5.4382#011validation-rmse:4.99133\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:4.68463#011validation-rmse:4.42345\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:4.03009#011validation-rmse:3.89612\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:3.53217#011validation-rmse:3.50541\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:3.17298#011validation-rmse:3.26247\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.89603#011validation-rmse:3.10463\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.68315#011validation-rmse:3.00372\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:2.50349#011validation-rmse:2.91563\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:2.38209#011validation-rmse:2.84303\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:2.26717#011validation-rmse:2.78905\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:2.15705#011validation-rmse:2.75432\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:2.09256#011validation-rmse:2.70906\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:2.00573#011validation-rmse:2.67592\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.92923#011validation-rmse:2.70137\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.89142#011validation-rmse:2.72597\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.85698#011validation-rmse:2.72608\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.80561#011validation-rmse:2.73383\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.76207#011validation-rmse:2.73436\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.73782#011validation-rmse:2.72873\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.64919#011validation-rmse:2.74729\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.61267#011validation-rmse:2.7742\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.5733#011validation-rmse:2.77287\u001b[0m\n",
      "\u001b[34m[07:22:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.5472#011validation-rmse:2.75555\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:2.00573#011validation-rmse:2.67592\n",
      "\u001b[0m\n",
      "\n",
      "2021-04-11 07:22:28 Completed - Training job completed\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "session.logs_for_job(training_job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Now that the training job has completed, we have some model artifacts which we can use to build a model. Note that here we mean SageMaker's definition of a model, which is a collection of information about a specific algorithm along with the artifacts which result from a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin by asking SageMaker to describe for us the results of the training job. The data structure\n",
    "# returned contains a lot more information than we currently need, try checking it out yourself in\n",
    "# more detail.\n",
    "\n",
    "### SageMaker model is a collection of information that includes both a link to the model artifacts, that is the saved files\n",
    "### created by the the trainning job, and some information that describes how those model artifacts should be used \n",
    "\n",
    "training_job_info = session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "### the model_artifacts which we can extract from the trainning job information, and some additional information about how those\n",
    "### model artifacts can be used\n",
    "model_artifacts = training_job_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like when we created a training job, the model name must be unique\n",
    "\n",
    "### just like when we created a trainning job, a model must have a name which is unique\n",
    "model_name = training_job_name + \"-model\" ### use the trainning_job_name + \"-model\"\n",
    "\n",
    "# We also need to tell SageMaker which container should be used for inference and where it should\n",
    "# retrieve the model artifacts from. In our case, the xgboost container that we used for training\n",
    "# can also be used for inference.\n",
    "\n",
    "### we package up informations including a link to\n",
    "primary_container = {\n",
    "    \"Image\": container, ### a link to the model docker container which contains the algorithm necessary to perform inference\n",
    "                        ### using our created model artifacts\n",
    "    \"ModelDataUrl\": model_artifacts  ### a linke to the model artifacts\n",
    "}\n",
    "\n",
    "# And lastly we construct the SageMaker model\n",
    "model_info = session.sagemaker_client.create_model(\n",
    "                                ModelName = model_name, ### name\n",
    "                                ExecutionRoleArn = role, ### role\n",
    "                                PrimaryContainer = primary_container) ### data structure which includes a link to the container\n",
    "                                                                      ### and a lonk to the model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing the model\n",
    "\n",
    "Now that we have fit our model to the training data, using the validation data to avoid overfitting, we can test our model. To do this we will make use of SageMaker's Batch Transform functionality. In other words, we need to set up and execute a batch transform job, similar to the way that we constructed the training job earlier.\n",
    "\n",
    "### Set up the batch transform job\n",
    "\n",
    "Just like when we were training our model, we first need to provide some information in the form of a data structure that describes the batch transform job which we wish to execute.\n",
    "\n",
    "We will only be using some of the options available here but to see some of the additional options please see the SageMaker documentation for [creating a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like in each of the previous steps, we need to make sure to name our job and the name should be unique.\n",
    "transform_job_name = 'boston-xgboost-batch-transform-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Now we construct the data structure which will describe the batch transform job.\n",
    "\n",
    "### we describe the transform job in detail\n",
    "transform_request = \\\n",
    "{\n",
    "    \"TransformJobName\": transform_job_name,\n",
    "    \n",
    "    # This is the name of the model that we created earlier.\n",
    "    \"ModelName\": model_name, ## choosing the model which we wish to use for the transformation job\n",
    "    \n",
    "    # This describes how many compute instances should be used at once. If you happen to be doing a very large\n",
    "    # batch transform job it may be worth running multiple compute instances at once.\n",
    "    \"MaxConcurrentTransforms\": 1, ### how many simultanous transformations will occur at any given time\n",
    "    \n",
    "    # This says how big each individual request sent to the model should be, at most. One of the things that\n",
    "    # SageMaker does in the background is to split our data up into chunks so that each chunks stays under\n",
    "    # this size limit.\n",
    "    \"MaxPayloadInMB\": 6, ### the maximum amount of data that is sent to our model at any given time\n",
    "    \n",
    "    # Sometimes we may want to send only a single sample to our endpoint at a time, however in this case each of\n",
    "    # the chunks that we send should contain multiple samples of our input data.\n",
    "    \"BatchStrategy\": \"MultiRecord\", ### we want to send multiple records in each chunk of our data(在每份数据中发送多条记录)\n",
    "    \n",
    "    # This next object describes where the output data should be stored. Some of the more advanced options which\n",
    "    # we don't cover here also describe how SageMaker should collect output from various batches.\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/batch-bransform/\".format(session.default_bucket(),prefix)\n",
    "    },  ### where we would like SageMaker to save the results of our batch transformed job\n",
    "    \n",
    "    # Here we describe our input data. Of course, we need to tell SageMaker where on S3 our input data is stored, in\n",
    "    # addition we need to detail the characteristics of our input data. In particular, since SageMaker may need to\n",
    "    # split our data up into chunks, it needs to know how the individual samples in our data file appear. In our\n",
    "    # case each line is its own sample and so we set the split type to 'line'. We also need to tell SageMaker what\n",
    "    # type of data is being sent, in this case csv, so that it can properly serialize the data.\n",
    "    \n",
    "    ### we should describe the inputs\n",
    "    \"TransformInput\": {\n",
    "        \"ContentType\": \"text/csv\", ### we are sending the CSV data\n",
    "        \"SplitType\": \"Line\", ### we can break the data up line by line\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\", ### where the data is located in S3\n",
    "                \"S3Uri\": test_location,  ### in this location\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # And lastly we tell SageMaker what sort of compute instance we would like it to use.\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the batch transform job\n",
    "\n",
    "Now that we have created the request data structure, it is time to ask SageMaker to set up and run our batch transform job. Just like in the previous steps, SageMaker performs these tasks in the background so that if we want to wait for the transform job to terminate (and ensure the job is progressing) we can ask SageMaker to wait of the transform job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker is now working on creating and excuting that transform job in the background\n",
    "transform_response = session.sagemaker_client.create_transform_job(**transform_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................!\n"
     ]
    }
   ],
   "source": [
    "### in oder to get some visual representation of what's going on, we need to call the wait_for_transform_job method\n",
    "transform_desc = session.wait_for_transform_job(transform_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results\n",
    "\n",
    "Now that the transform job has completed, the results are stored on S3 as we requested. Since we'd like to do a bit of analysis in the notebook we can use some notebook magic to copy the resulting output from S3 and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we make sure to copy the output from our transform job to the local notebook instance\n",
    "transform_output = \"s3://{}/{}/batch-bransform/\".format(session.default_bucket(),prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-681803400601/boston-xgboost-LL/batch-bransform/test.csv.out to ../data/boston/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $transform_output $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our model works we can create a simple scatter plot between the predicted and actual values. If the model was completely accurate the resulting scatter plot would look like the line $x=y$. As we can see, our model seems to have done okay but there is room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Median Price vs Predicted Price')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Median Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Median Price vs Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Clean up\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As you continue to complete and execute notebooks you will eventually fill up this disk space, leading to errors which can be difficult to diagnose. Once you are completely finished using a notebook it is a good idea to remove the files that you created along the way. Of course, you can do this from the terminal or from the notebook hub if you would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
